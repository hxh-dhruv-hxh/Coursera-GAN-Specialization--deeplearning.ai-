{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2f555b",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab58051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(3, 32, 32), nrow=5, show=True):\n",
    "    \n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    \n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821facce",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b6f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (CIFAR100 is in color (red, green, blue), so 3 is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        \n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        \n",
    "        if not final_layer:\n",
    "            \n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        \n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    \n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    \n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "def combine_vectors(x, y):\n",
    "    \n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?)\n",
    "    Parameters:\n",
    "    x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "    y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    return torch.cat([x, y], 1)\n",
    "\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    \n",
    "    return F.one_hot(labels, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7996e3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3937672",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_shape = (3, 32, 32)\n",
    "n_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3962c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb338595",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim = z_dim + n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e15f3",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0456ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Classifier Class\n",
    "    Values:\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, im_chan, n_classes, hidden_dim=32):\n",
    "        \n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_classifier_block(im_chan, hidden_dim),\n",
    "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4),\n",
    "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True)\n",
    "        )\n",
    "        \n",
    "    def make_classifier_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        \n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a classifier block; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        \n",
    "        if not final_layer:\n",
    "            \n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=False)\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
    "            )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        '''\n",
    "        Function for completing a forward pass of the classifier: Given an image tensor, \n",
    "        returns an n_classes-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with im_chan channels\n",
    "        '''\n",
    "        \n",
    "        class_pred = self.disc(image)\n",
    "        return class_pred.view(len(class_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce62aba",
   "metadata": {},
   "source": [
    "### Tuning the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac296b3",
   "metadata": {},
   "source": [
    "You will fine-tune your model by augmenting the original real data with fake data and during that process, observe how to increase the accuracy of your classifier with these fake, GAN-generated bugs. After this, you will prove your worth as a bug master.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298a46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sample(real, fake, p_real):\n",
    "    \n",
    "    '''\n",
    "    Function to take a set of real and fake images of the same length (x)\n",
    "    and produce a combined tensor with length (x) and sampled at the target probability\n",
    "    Parameters:\n",
    "        real: a tensor of real images, length (x)\n",
    "        fake: a tensor of fake images, length (x)\n",
    "        p_real: the probability the images are sampled from the real set\n",
    "    '''\n",
    "    \n",
    "    make_fake = torch.rand(len(real)) > p_real\n",
    "    target_images = real.clone()\n",
    "    target_images[make_fake] = fake[make_fake]\n",
    "    \n",
    "    return target_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fddedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "c = torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8452f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074])\n",
      "tensor([0.6341, 0.4901, 0.8964, 0.4556, 0.6323])\n"
     ]
    }
   ],
   "source": [
    "b = [True, False, True, False, True]\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583853f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4963, 0.4901, 0.0885, 0.4556, 0.3074])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[b] = a[b]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d83703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "n_test_samples = 9999\n",
    "test_combination = combine_sample(\n",
    "    torch.ones(n_test_samples, 1), \n",
    "    torch.zeros(n_test_samples, 1), \n",
    "    0.3\n",
    ")\n",
    "# Check that the shape is right\n",
    "assert tuple(test_combination.shape) == (n_test_samples, 1)\n",
    "# Check that the ratio is right\n",
    "assert torch.abs(test_combination.mean() - 0.3) < 0.05\n",
    "# Make sure that no mixing happened\n",
    "assert test_combination.median() < 1e-5\n",
    "\n",
    "test_combination = combine_sample(\n",
    "    torch.ones(n_test_samples, 10, 10), \n",
    "    torch.zeros(n_test_samples, 10, 10), \n",
    "    0.8\n",
    ")\n",
    "# Check that the shape is right\n",
    "assert tuple(test_combination.shape) == (n_test_samples, 10, 10)\n",
    "# Make sure that no mixing happened\n",
    "assert torch.abs((test_combination.sum([1, 2]).median()) - 100) < 1e-5\n",
    "\n",
    "test_reals = torch.arange(n_test_samples)[:, None].float()\n",
    "test_fakes = torch.zeros(n_test_samples, 1)\n",
    "test_saved = (test_reals.clone(), test_fakes.clone())\n",
    "test_combination = combine_sample(test_reals, test_fakes, 0.3)\n",
    "# Make sure that the sample isn't biased\n",
    "assert torch.abs((test_combination.mean() - 1500)) < 100\n",
    "# Make sure no inputs were changed\n",
    "assert torch.abs(test_saved[0] - test_reals).sum() < 1e-3\n",
    "assert torch.abs(test_saved[1] - test_fakes).sum() < 1e-3\n",
    "\n",
    "test_fakes = torch.arange(n_test_samples)[:, None].float()\n",
    "test_combination = combine_sample(test_reals, test_fakes, 0.3)\n",
    "# Make sure that the order is maintained\n",
    "assert torch.abs(test_combination - test_reals).sum() < 1e-4\n",
    "if torch.cuda.is_available():\n",
    "    # Check that the solution matches the input device\n",
    "    assert str(combine_sample(\n",
    "        torch.ones(n_test_samples, 10, 10).cuda(), \n",
    "        torch.zeros(n_test_samples, 10, 10).cuda(),\n",
    "        0.8\n",
    "    ).device).startswith(\"cuda\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05684ace",
   "metadata": {},
   "source": [
    "Now you have a challenge: find a p_real and a generator image such that your classifier gets an average of a 51% accuracy or higher on the insects, when evaluated with the eval_augmentation function. You'll need to fill in find_optimal to find these parameters to solve this part!\n",
    "\n",
    "When you're training a generator, you will often have to look at different checkpoints and choose one that does the best (either empirically or using some evaluation method). Here, you are given four generator checkpoints: gen_1.pt, gen_2.pt, gen_3.pt, gen_4.pt. You'll also have some scratch area to write whatever code you'd like to solve this problem, but you must return a p_real and an image name of your selected generator checkpoint. You can hard-code/brute-force these numbers if you would like, but you are encouraged to try to solve this problem in a more general way. In practice, you would also want a test set (since it is possible to overfit on a validation set), but for simplicity you can just focus on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d6b930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb61ab4357f64fbd8b834d42f45db5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"gen.0.0.weight\", \"gen.0.0.bias\", \"gen.0.1.weight\", \"gen.0.1.bias\", \"gen.0.1.running_mean\", \"gen.0.1.running_var\", \"gen.0.1.num_batches_tracked\", \"gen.1.0.weight\", \"gen.1.0.bias\", \"gen.1.1.weight\", \"gen.1.1.bias\", \"gen.1.1.running_mean\", \"gen.1.1.running_var\", \"gen.1.1.num_batches_tracked\", \"gen.2.0.weight\", \"gen.2.0.bias\", \"gen.2.1.weight\", \"gen.2.1.bias\", \"gen.2.1.running_mean\", \"gen.2.1.running_var\", \"gen.2.1.num_batches_tracked\", \"gen.3.0.weight\", \"gen.3.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4a898516b636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m \u001b[0mbest_p_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_gen_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_optimal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[0mperformance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_p_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_gen_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4a898516b636>\u001b[0m in \u001b[0;36mfind_optimal\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp_real\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_real_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mcurr_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurr_eval\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_eval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4a898516b636>\u001b[0m in \u001b[0;36meval_augmentation\u001b[1;34m(p_real, gen_name, n_test)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maugmented_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4a898516b636>\u001b[0m in \u001b[0;36maugmented_train\u001b[1;34m(p_real, gen_name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_input_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar100_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\specter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"gen.0.0.weight\", \"gen.0.0.bias\", \"gen.0.1.weight\", \"gen.0.1.bias\", \"gen.0.1.running_mean\", \"gen.0.1.running_var\", \"gen.0.1.num_batches_tracked\", \"gen.1.0.weight\", \"gen.1.0.bias\", \"gen.1.1.weight\", \"gen.1.1.bias\", \"gen.1.1.running_mean\", \"gen.1.1.running_var\", \"gen.1.1.num_batches_tracked\", \"gen.2.0.weight\", \"gen.2.0.bias\", \"gen.2.1.weight\", \"gen.2.1.bias\", \"gen.2.1.running_mean\", \"gen.2.1.running_var\", \"gen.2.1.num_batches_tracked\", \"gen.3.0.weight\", \"gen.3.0.bias\". "
     ]
    }
   ],
   "source": [
    "def find_optimal():\n",
    "    \n",
    "    gen_names = [\n",
    "        \"gen_1.pt\",\n",
    "        \"gen_2.pt\",\n",
    "        \"gen_3.pt\",\n",
    "        \"gen_4.pt\"\n",
    "    ]\n",
    "    \n",
    "    best_p_real, best_gen_name = 0, \"gen_1.pt\"\n",
    "    max_eval = -1\n",
    "    \n",
    "    for gen_name in gen_names:\n",
    "        \n",
    "        p_real_all = torch.linspace(0, 1, 21)\n",
    "        \n",
    "        for p_real in tqdm(p_real_all):\n",
    "            \n",
    "            curr_eval = eval_augmentation(p_real, gen_name, n_test=20)\n",
    "            \n",
    "            if curr_eval > max_eval:\n",
    "                \n",
    "                max_eval = curr_eval\n",
    "                best_p_real = p_real\n",
    "                best_gen_name = gen_name\n",
    "                \n",
    "    return best_p_real, best_gen_name\n",
    "\n",
    "def augmented_train(p_real, gen_name):\n",
    "    \n",
    "    gen = Generator(generator_input_dim).to(device)\n",
    "    gen.load_state_dict(torch.load(gen_name))\n",
    "    \n",
    "    classifier = Classifier(cifar100_shape[0], n_classes).to(device)\n",
    "    classifier.load_state_dict(torch.load('class.pt'))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    batch_size = 256\n",
    "    \n",
    "    train_set = torch.load('insect_train.pt')\n",
    "    val_set = torch.load('insect_val.pt')\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        torch.utils.data.TensorDataset(train_set[\"images\"], train_set['labels']),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    validation_dataloader = DataLoader(\n",
    "        torch.utils.data.TensorDataset(val_set['images'], val_set['labels']),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    display_step = 1\n",
    "    lr = 0.0002\n",
    "    n_epochs = 20\n",
    "    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
    "    cur_step = 0\n",
    "    best_score = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for real, labels in dataloader:\n",
    "            \n",
    "            real = real.to(device)\n",
    "            \n",
    "            # Flatten the image\n",
    "            labels = labels.to(device)\n",
    "            one_hot_labels = get_one_hot_labels(labels, n_classes).float()\n",
    "            \n",
    "            # Updating the classifier\n",
    "            classifier_opt.zero_grad()\n",
    "            cur_batch_size = len(labels)\n",
    "            \n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "            fake = gen(noise_and_labels)\n",
    "            \n",
    "            target_images = combine_sample(real.clone(), fake.clone(), p_real)\n",
    "            labels_hat = classifier(target_images.detach())\n",
    "            classifier_loss = criterion(labels_hat, labels)\n",
    "            classifier_loss.backward()\n",
    "            classifier_opt.step()\n",
    "            \n",
    "            # Calculate the accuracy of the validation set\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                \n",
    "                classifier_val_loss = 0\n",
    "                classifier_correct = 0\n",
    "                num_validation = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    for val_example, val_label in validation_dataloader:\n",
    "                        \n",
    "                        cur_batch_size = len(val_example)\n",
    "                        num_validation += cur_batch_size\n",
    "                        val_example = val_example.to(device)\n",
    "                        val_label = val_label.to(device)\n",
    "                        labels_hat = Classifier(val_example)\n",
    "                        classifier_val_loss += criterion(labels_hat, labels) * cur_batch_size\n",
    "                        classifier_correct += (labels_hat.argmax(1) == val_label).float().sum()\n",
    "                        \n",
    "                    accuracy = classifier_correct.item() / num_validation\n",
    "                    \n",
    "                    if accuracy > best_score:\n",
    "                        \n",
    "                        best_score = accuracy\n",
    "                        \n",
    "            cur_step += 1\n",
    "            \n",
    "    return best_score\n",
    "\n",
    "def eval_augmentation(p_real, gen_name, n_test=20):\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        \n",
    "        total += augmented_train(p_real, gen_name)\n",
    "        \n",
    "    return total / n_test\n",
    "\n",
    "best_p_real, best_gen_name = find_optimal()\n",
    "\n",
    "performance = eval_augmentation(best_p_real, best_gen_name)\n",
    "\n",
    "print(f\"Your model had an accuracy of {performance:0.1%}\")\n",
    "assert performance > 0.51\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e3c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d8a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e8d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13be20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fb87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb51b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229a54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf0bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c809ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a2b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a5937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c7693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c9568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcfe6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5058b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826d85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c80af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df075bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "specter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
